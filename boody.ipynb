{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y, pad):\n",
    "    \"\"\"\n",
    "    This is the constructor of the NERDataset\n",
    "    Inputs:\n",
    "    - x: a list of lists where each list contains the ids of the tokens\n",
    "    - y: a list of lists where each list contains the label of each token in the sentence\n",
    "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
    "    \"\"\"\n",
    "    ##################### TODO: create two tensors one for x and the other for labels ###############################\n",
    "    # padding all sentences and labels to have the same length\n",
    "    max_len = max([len(sentence) for sentence in x])\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "      x[i] = torch.cat((x[i], torch.zeros(max_len - len(x[i]), 768)), 0) \n",
    "      y[i] = y[i] + [0] * (max_len - len(y[i]))\n",
    "      \n",
    "    self.x = torch.stack(x)\n",
    "    self.y = torch.tensor(y)\n",
    "    #################################################################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    This function should return the length of the dataset (the number of sentences)\n",
    "    \"\"\"\n",
    "    ###################### TODO: return the length of the dataset #############################\n",
    "    return len(self.x)\n",
    "    ###########################################################################################\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    This function returns a subset of the whole dataset\n",
    "    \"\"\"\n",
    "    ###################### TODO: return a tuple of x and y ###################################\n",
    "    return self.x[idx], self.y[idx]\n",
    "    ##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context_Model(nn.Module):\n",
    "  def __init__(self, input_size=768, hidden_size=50, n_classes=100, bidirectional=False):\n",
    "    \"\"\"\n",
    "    The constructor of our NER model\n",
    "    Inputs:\n",
    "    - vacab_size: the number of unique words\n",
    "    - embedding_dim: the embedding dimension\n",
    "    - n_classes: the number of final classes (tags)\n",
    "    \"\"\"\n",
    "    super(Context_Model, self).__init__()\n",
    "    ####################### TODO: Create the layers of your model #######################################\n",
    "    # (1) Create the embedding layer\n",
    "    \n",
    "\n",
    "    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "    # (3) Create a linear layer with number of neorons = n_classes\n",
    "    self.linear = nn.Linear(hidden_size * (2 if bidirectional else 1), n_classes)\n",
    "    #####################################################################################################\n",
    "\n",
    "  def forward(self, X, hidden=None):\n",
    "    \"\"\"\n",
    "    This function does the forward pass of our model\n",
    "    Inputs:\n",
    "    - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "    Returns:\n",
    "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "    \"\"\"\n",
    "\n",
    "    ######################### TODO: implement the forward pass ####################################\n",
    "    final_output, hidden = self.lstm(X, hidden)\n",
    "    final_output = self.linear(final_output)\n",
    "    ###############################################################################################\n",
    "    return final_output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, n_classes, batch_size=512, epochs=5, learning_rate=0.01):\n",
    "  \"\"\"\n",
    "  This function implements the training logic\n",
    "  Inputs:\n",
    "  - model: the model ot be trained\n",
    "  - train_dataset: the training set of type NERDataset\n",
    "  - batch_size: integer represents the number of examples per step\n",
    "  - epochs: integer represents the total number of epochs (full training pass)\n",
    "  - learning_rate: the learning rate to be used by the optimizer\n",
    "  \"\"\"\n",
    "\n",
    "  ############################## TODO: replace the Nones in the following code ##################################\n",
    "  \n",
    "  # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  # (2) make the criterion cross entropy loss\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "  # (3) create the optimizer (Adam)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # GPU configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "      # (4) move the train input to the device\n",
    "      train_input = train_input.to(device)\n",
    "\n",
    "      # (5) move the train label to the device\n",
    "      train_label = train_label.to(device)\n",
    "\n",
    "\n",
    "      # (6) do the forward pass\n",
    "      output, _ = model(train_input)\n",
    "      output = output.to(device)\n",
    "      \n",
    "      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "      # -1 is ignore \n",
    "      batch_loss = criterion(output.view(-1, n_classes), train_label.view(-1))\n",
    "    \n",
    "\n",
    "      # (8) append the batch loss to the total_loss_train\n",
    "      total_loss_train += batch_loss.item()\n",
    "      \n",
    "      # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "      acc = (output.argmax(dim=-1) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "\n",
    "      # (10) zero your gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # (11) do the backward pass\n",
    "      batch_loss.backward(retain_graph=True)\n",
    "\n",
    "      # (12) update the weights with your optimizer\n",
    "      optimizer.step()\n",
    "      \n",
    "    # epoch loss\n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "    # (13) calculate the accuracy\n",
    "    epoch_acc = total_acc_train / (len(train_dataset) * train_dataset[0][0].shape[0])\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "        | Train Accuracy: {epoch_acc}\\n')\n",
    "\n",
    "  ##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2word ...\n",
      "word2id ...\n",
      "starting embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 0.9554011344909668         | Train Accuracy: 0.0017699115044247787\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 0.8444215774536132         | Train Accuracy: 0.7238938053097345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 0.2575518608093262         | Train Accuracy: 0.7451327433628319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./processed/val.csv\")\n",
    "\n",
    "data = df[\"tokenized_cleaned\"].apply(eval)\n",
    "data = list(filter(lambda sent: len(sent) > 1, data))\n",
    "id2word = {idx+1:word for idx, word in  enumerate(set(itertools.chain.from_iterable(data)))}\n",
    "id2word[0] = '<PAD>'\n",
    "print(\"id2word ...\")\n",
    "\n",
    "word2id = {word:idx for idx, word in id2word.items()}\n",
    "print(\"word2id ...\")\n",
    "\n",
    "context_model = Context_Model(hidden_size=256, n_classes=len(word2id.keys()), bidirectional=True).to(device)\n",
    "\n",
    "X  = pd.DataFrame()\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('CAMeL-Lab/bert-base-arabic-camelbert-ca')\n",
    "embedder = AutoModel.from_pretrained('CAMeL-Lab/bert-base-arabic-camelbert-ca')\n",
    "X[0] = [tokenizer(sent[:-1], return_tensors=\"pt\", padding=True) for sent in data]\n",
    "\n",
    "print(\"starting embedding...\")\n",
    "\n",
    "tqdm.pandas()\n",
    "train_data = []\n",
    "for i in range(0, len(X[0]), 500):\n",
    "  batch = X[0][i:i+500].progress_apply(lambda x: embedder(**x).last_hidden_state[:,1, :])\n",
    "  train_data.extend(batch)\n",
    "Y = [ [word2id[word] for word in sent[1:]] for sent in data]\n",
    "print(\"starting training...\")\n",
    "\n",
    "train_dataset = Dataset(train_data, Y, np.zeros(768))\n",
    "\n",
    "train(context_model, train_dataset, n_classes=len(word2id.keys()), batch_size=20, epochs=3, learning_rate=0.01)\n",
    "\n",
    "pickle.dump(context_model, open(\"./models/context_model_bi.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
