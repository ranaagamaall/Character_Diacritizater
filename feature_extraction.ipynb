{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\\n</td>\n",
       "      <td>[قوله, ولا, تكره, ضيافته]</td>\n",
       "      <td>قوله ولا تكره ضيافته</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ ...</td>\n",
       "      <td>[الفرق, الثالث, والثلاثون, بين, قاعدة, تقدم, ا...</td>\n",
       "      <td>الفرق الثالث والثلاثون بين قاعدة تقدم الحكم عل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>( قَوْلُهُ : وَهُوَ ) أَيْ : الْبَيْعُ بِالْمَ...</td>\n",
       "      <td>[قوله, وهو, أي, البيع, بالمعنى, الثاني, الذي, ...</td>\n",
       "      <td>قوله وهو أي البيع بالمعنى الثاني الذي هو العقد...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>وَالْعَفْوُ قَبْلَ الْإِمَامِ ، أَوْ بَعْدَهُ ...</td>\n",
       "      <td>[والعفو, قبل, الإمام, ،, أو, بعده, ،, إن, أراد...</td>\n",
       "      <td>والعفو قبل الإمام ، أو بعده ، إن أراد سترا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>( قَوْلُهُ : وَرِبْحُهُ ) أَيْ الْقِرَاضِ وَقَ...</td>\n",
       "      <td>[قوله, وربحه, أي, القراض, وقوله, كخراجه, أي, أ...</td>\n",
       "      <td>قوله وربحه أي القراض وقوله كخراجه أي أجرة خدمت...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0   \n",
       "0   ( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\\n  \\\n",
       "1  ( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ ...   \n",
       "2  ( قَوْلُهُ : وَهُوَ ) أَيْ : الْبَيْعُ بِالْمَ...   \n",
       "3  وَالْعَفْوُ قَبْلَ الْإِمَامِ ، أَوْ بَعْدَهُ ...   \n",
       "4  ( قَوْلُهُ : وَرِبْحُهُ ) أَيْ الْقِرَاضِ وَقَ...   \n",
       "\n",
       "                                           tokenized   \n",
       "0                          [قوله, ولا, تكره, ضيافته]  \\\n",
       "1  [الفرق, الثالث, والثلاثون, بين, قاعدة, تقدم, ا...   \n",
       "2  [قوله, وهو, أي, البيع, بالمعنى, الثاني, الذي, ...   \n",
       "3  [والعفو, قبل, الإمام, ،, أو, بعده, ،, إن, أراد...   \n",
       "4  [قوله, وربحه, أي, القراض, وقوله, كخراجه, أي, أ...   \n",
       "\n",
       "                                             cleaned  \n",
       "0                               قوله ولا تكره ضيافته  \n",
       "1  الفرق الثالث والثلاثون بين قاعدة تقدم الحكم عل...  \n",
       "2  قوله وهو أي البيع بالمعنى الثاني الذي هو العقد...  \n",
       "3         والعفو قبل الإمام ، أو بعده ، إن أراد سترا  \n",
       "4  قوله وربحه أي القراض وقوله كخراجه أي أجرة خدمت...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./processed/val.csv\")\n",
    "df[\"tokenized\"] = df[\"tokenized\"].apply(eval)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\\n</td>\n",
       "      <td>[قوله, ولا, تكره, ضيافته]</td>\n",
       "      <td>قوله ولا تكره ضيافته</td>\n",
       "      <td>[[[[[0. 0. 0. ... 0. 0. 0.]]]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ ...</td>\n",
       "      <td>[الفرق, الثالث, والثلاثون, بين, قاعدة, تقدم, ا...</td>\n",
       "      <td>الفرق الثالث والثلاثون بين قاعدة تقدم الحكم عل...</td>\n",
       "      <td>[[[[[0. 0. 0. ... 0. 0. 0.]]]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>( قَوْلُهُ : وَهُوَ ) أَيْ : الْبَيْعُ بِالْمَ...</td>\n",
       "      <td>[قوله, وهو, أي, البيع, بالمعنى, الثاني, الذي, ...</td>\n",
       "      <td>قوله وهو أي البيع بالمعنى الثاني الذي هو العقد...</td>\n",
       "      <td>[[[[[0. 0. 0. ... 0. 0. 0.]]]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>وَالْعَفْوُ قَبْلَ الْإِمَامِ ، أَوْ بَعْدَهُ ...</td>\n",
       "      <td>[والعفو, قبل, الإمام, ،, أو, بعده, ،, إن, أراد...</td>\n",
       "      <td>والعفو قبل الإمام ، أو بعده ، إن أراد سترا</td>\n",
       "      <td>[[[[[0. 0. 0. ... 0. 0. 0.]]]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>( قَوْلُهُ : وَرِبْحُهُ ) أَيْ الْقِرَاضِ وَقَ...</td>\n",
       "      <td>[قوله, وربحه, أي, القراض, وقوله, كخراجه, أي, أ...</td>\n",
       "      <td>قوله وربحه أي القراض وقوله كخراجه أي أجرة خدمت...</td>\n",
       "      <td>[[[[[0. 0. 0. ... 0. 0. 0.]]]]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0   \n",
       "0   ( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\\n  \\\n",
       "1  ( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ ...   \n",
       "2  ( قَوْلُهُ : وَهُوَ ) أَيْ : الْبَيْعُ بِالْمَ...   \n",
       "3  وَالْعَفْوُ قَبْلَ الْإِمَامِ ، أَوْ بَعْدَهُ ...   \n",
       "4  ( قَوْلُهُ : وَرِبْحُهُ ) أَيْ الْقِرَاضِ وَقَ...   \n",
       "\n",
       "                                           tokenized   \n",
       "0                          [قوله, ولا, تكره, ضيافته]  \\\n",
       "1  [الفرق, الثالث, والثلاثون, بين, قاعدة, تقدم, ا...   \n",
       "2  [قوله, وهو, أي, البيع, بالمعنى, الثاني, الذي, ...   \n",
       "3  [والعفو, قبل, الإمام, ،, أو, بعده, ،, إن, أراد...   \n",
       "4  [قوله, وربحه, أي, القراض, وقوله, كخراجه, أي, أ...   \n",
       "\n",
       "                                             cleaned   \n",
       "0                               قوله ولا تكره ضيافته  \\\n",
       "1  الفرق الثالث والثلاثون بين قاعدة تقدم الحكم عل...   \n",
       "2  قوله وهو أي البيع بالمعنى الثاني الذي هو العقد...   \n",
       "3         والعفو قبل الإمام ، أو بعده ، إن أراد سترا   \n",
       "4  قوله وربحه أي القراض وقوله كخراجه أي أجرة خدمت...   \n",
       "\n",
       "                            tf_idf  \n",
       "0  [[[[[0. 0. 0. ... 0. 0. 0.]]]]]  \n",
       "1  [[[[[0. 0. 0. ... 0. 0. 0.]]]]]  \n",
       "2  [[[[[0. 0. 0. ... 0. 0. 0.]]]]]  \n",
       "3  [[[[[0. 0. 0. ... 0. 0. 0.]]]]]  \n",
       "4  [[[[[0. 0. 0. ... 0. 0. 0.]]]]]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(sent):\n",
    "    return tfidf_vectorizer.transform([sent]).todense()\n",
    "\n",
    "df[\"tf_idf\"] = df[\"cleaned\"].apply(extract_features)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./processed/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.154773690669971"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tf_idf\"][1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre and post characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_vectors(text, window_size=1):\n",
    "    \"\"\"\n",
    "    Creates input vectors for a character-level LSTM model,\n",
    "    incorporating pre and post character features.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be processed.\n",
    "        window_size (int, optional): The number of characters to include\n",
    "            on each side of the target character. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of input vectors, where each vector is a NumPy array\n",
    "            representing the target character, its previous characters,\n",
    "            and its following characters.\n",
    "    \"\"\"\n",
    "\n",
    "    input_vectors = []\n",
    "    for i in range(len(text)):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(text), i + window_size + 1)\n",
    "        context = text[start:end]\n",
    "        target_char = text[i]\n",
    "        input_vector = np.zeros(3 * (window_size * 2 + 1))  # Assuming one-hot encoding\n",
    "        input_vector[window_size * 2] = 1  # Indicate target character\n",
    "        # Set one-hot encodings for surrounding characters (replace with embeddings if needed)\n",
    "        for j, char in enumerate(context):\n",
    "            index = window_size - j if j < window_size else window_size + j + 1\n",
    "            input_vector[index] = 1\n",
    "        input_vectors.append(input_vector)\n",
    "    return input_vectors\n",
    "\n",
    "def create_bef_after(text):\n",
    "    input_vectors = []\n",
    "    for i in range(len(text)):\n",
    "        letter_vec = []\n",
    "        if i == 0:\n",
    "            letter_vec.append(36)\n",
    "            letter_vec.append(arabic_letters.index(text[i+1]))\n",
    "            input_vectors.append(letter_vec)\n",
    "            continue\n",
    "        elif i == len(text)-1:\n",
    "            letter_vec.append(arabic_letters.index(text[i-1]))\n",
    "\n",
    "            letter_vec.append(36)\n",
    "            input_vectors.append(letter_vec)\n",
    "            break\n",
    "        letter_vec.append(arabic_letters.index(text[i-1]))\n",
    "\n",
    "        letter_vec.append(arabic_letters.index(text[i+1]))\n",
    "        input_vectors.append(letter_vec)\n",
    "    \n",
    "    \n",
    "    return input_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diacritics_probability_per_char(tokenized_cleaned, diacritics): \n",
    "    probability = {}\n",
    "    \n",
    "    for i in range(36):\n",
    "        probability[i] = {}\n",
    "        for j in range(15):\n",
    "            probability[i][j] = 0\n",
    "            \n",
    "    counts = [0]*36\n",
    "            \n",
    "    for i in range(tokenized_cleaned):\n",
    "        for j in range(len(tokenized_cleaned[i])):\n",
    "            probability[arabic_letters.index(tokenized_cleaned[i][j])][diacritics[i][j]] += 1\n",
    "            counts[arabic_letters.index(tokenized_cleaned[i][j])] += 1\n",
    "            \n",
    "            \n",
    "    for i in range(36):\n",
    "        for j in range(15):\n",
    "            probability[i][j] /= counts[i]\n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "arabic_letters = list(pickle.load(\n",
    "        open(\"./assets/arabic_letters.pickle\", \"rb\")))\n",
    "arabic_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[36, 33],\n",
       " [25, 11],\n",
       " [33, 2],\n",
       " [11, 11],\n",
       " [2, 33],\n",
       " [11, 2],\n",
       " [33, 25],\n",
       " [2, 30],\n",
       " [25, 36]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_bef_after(\"والثلاثون\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
